{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9t7TRydCmhUBXSxxSAHCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Burnhard/Iris-classification/blob/main/Iris(Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-U1gZ57WvInF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading or importing  wine dataset to notebook\n",
        "cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "iris_data = pd.read_csv(\"iris.data\", names=cols)\n",
        "iris_data.head()"
      ],
      "metadata": {
        "id": "XAWSgWqQ6Q9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "1JKtfVgd7sFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying missing values**"
      ],
      "metadata": {
        "id": "UBeKHqIi7v63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values\n",
        "missing_values = iris_data.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "qw4DiXLf7rHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying duplicates**"
      ],
      "metadata": {
        "id": "zyqh_Z9V8S6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify duplicate rows\n",
        "duplicates = iris_data.duplicated()\n",
        "print(\"Duplicates instances: \\n\",duplicates)"
      ],
      "metadata": {
        "id": "BTu4VAo68W7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Detection using Boxplot**"
      ],
      "metadata": {
        "id": "JPLGJjCG8e5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = iris_data\n",
        "\n",
        "# Visualize the distribution of each feature using box plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features')\n",
        "plt.show()\n",
        "\n",
        "# Identify outliers using statistical methods (e.g., Z-score or IQR)\n",
        "# Z-score method\n",
        "from scipy.stats import zscore\n",
        "\n",
        "z_scores = zscore(data)\n",
        "outlier_threshold = 3  # Adjust the threshold as per your preference\n",
        "outliers = (abs(z_scores) > outlier_threshold).any(axis=1)\n",
        "\n",
        "# IQR method\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "\n",
        "# Count the number of outliers\n",
        "num_outliers = outliers.sum()\n",
        "print(f\"Number of outliers: {num_outliers}\")\n",
        "\n",
        "# Decide whether to remove outliers or transform them\n",
        "remove_outliers = False\n",
        "\n",
        "if remove_outliers:\n",
        "    # Remove outliers from the dataset\n",
        "    data = data[~outliers]\n",
        "    print(\"Outliers removed.\")\n",
        "else:\n",
        "    # Transform outliers to a specific value\n",
        "    outlier_value = 8  # Choose an appropriate value for transformation\n",
        "    data[outliers] = outlier_value\n",
        "    print(\"Outliers transformed.\")\n",
        "\n",
        "# Updated visualization after handling outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features (After Outlier Handling)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pNkJaWherNlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Influencial datapoint detection using leverage and cooks distance**"
      ],
      "metadata": {
        "id": "c60qet_JK7nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(iris_data)"
      ],
      "metadata": {
        "id": "PeoDgruxsiE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable (class) from the features\n",
        "iris_data = pd.read_csv(\"iris.data\", names=cols)\n",
        "\n",
        "X = iris_data.drop('class', axis=1)\n",
        "y = iris_data['class']\n",
        "\n",
        "# Convert the target variable to numeric labels\n",
        "label_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
        "y = y.map(label_mapping)\n"
      ],
      "metadata": {
        "id": "Mh906YSkrbp-"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add a constant term to the features matrix for the intercept in the linear regression model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X)\n",
        "results = model.fit()\n",
        "\n",
        "# Get the influence statistics\n",
        "influence = OLSInfluence(results)\n",
        "\n",
        "# Calculate the leverage values\n",
        "leverage = influence.hat_matrix_diag\n",
        "\n",
        "# Calculate the Cook's distance\n",
        "cooks_distance = influence.cooks_distance\n",
        "\n",
        "# Identify influential data points based on leverage or Cook's distance\n",
        "influential_points_leverage = leverage > 2 * (X.shape[1] + 1) / X.shape[0]\n",
        "influential_points_cooks = cooks_distance[0] > 4 / (X.shape[0] - X.shape[1] - 1)\n",
        "\n",
        "# Print the influential data points\n",
        "print(\"Influential points based on leverage:\")\n",
        "print(X[influential_points_leverage])\n",
        "\n",
        "print(\"\\nInfluential points based on Cook's distance:\")\n",
        "print(X[influential_points_cooks])\n"
      ],
      "metadata": {
        "id": "MJVCu88ALouA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normality of the set of features using shapiro**"
      ],
      "metadata": {
        "id": "1KhOoo-7N3NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "\n",
        "\n",
        "# Select the features to check for normality\n",
        "features = X\n",
        "\n",
        "# Perform Shapiro-Wilk test for each feature\n",
        "for column in features.columns:\n",
        "    stat, p_value = shapiro(features[column])\n",
        "    alpha = 0.05  # Significance level\n",
        "\n",
        "    print(f\"Feature: {column}\")\n",
        "    print(f\"Shapiro-Wilk test statistic: {stat}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "\n",
        "    if p_value > alpha:\n",
        "        print(\"Feature appears to be normally distributed.\")\n",
        "    else:\n",
        "        print(\"Feature does not appear to be normally distributed.\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "QyRPVnOBOSZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transformation**"
      ],
      "metadata": {
        "id": "OHa8Ye1yPn6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "X,y\n",
        "\n",
        "# Separate the target variable (class) from the features\n",
        "\n",
        "# Perform normalization using Min-Max scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Perform standardization using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "Ifo3Jf1WP18f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n"
      ],
      "metadata": {
        "id": "IigvJeNHTlJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# Create a Random Forest regressor\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance (descending order)\n",
        "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(feature_importances_df)"
      ],
      "metadata": {
        "id": "QC-xb_niUM_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling techniques using the Synthetic Minority Over-sampling Technique (SMOTE) to balance the imbalanced dataset**"
      ],
      "metadata": {
        "id": "_sFgOUcNk210"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE()\n",
        "\n",
        "# Perform oversampling\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the balanced class distribution\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "print(y_resampled.value_counts())"
      ],
      "metadata": {
        "id": "U4vDa3dylbh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting Appropriate Learners for Training and Validation (Random Forest  and Support Vector Machine)**"
      ],
      "metadata": {
        "id": "Qx4jUH3EoXkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "IGJuxV6TqGZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Generate a classification report\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report)\n"
      ],
      "metadata": {
        "id": "71kvhLb8pWaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Machines (SVM)**"
      ],
      "metadata": {
        "id": "3tU_yhu0xvBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut, KFold\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# K-fold cross-validation\n",
        "kfold = KFold(n_splits=5)\n",
        "svm_scores_kfold = cross_val_score(svm, X, y, cv=kfold)\n",
        "\n",
        "# Leave-one-out cross-validation\n",
        "loo = LeaveOneOut()\n",
        "svm_scores_loo = cross_val_score(svm, X, y, cv=loo)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Support Vector Machines (SVM) - K-fold Cross-validation scores:\\n\")\n",
        "print(svm_scores_kfold)\n",
        "\n",
        "print(\"\\nSupport Vector Machines (SVM) - Leave-one-out Cross-validation scores:\\n\")\n",
        "print(svm_scores_loo)\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "id": "dLlizKkryCej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best Model Learner**"
      ],
      "metadata": {
        "id": "YYB5knPW8Wvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create the SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the SVM classifier\n",
        "svm_predictions = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for the SVM classifier\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the Random Forest classifier\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for the Random Forest classifier\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "\n",
        "# Compare accuracies and select the best model/learner\n",
        "if svm_accuracy > rf_accuracy:\n",
        "    best_model = svm_classifier\n",
        "    best_accuracy = svm_accuracy\n",
        "    model_name = \"SVM\"\n",
        "\n",
        "elif svm_accuracy == rf_accuracy:\n",
        "    best_model = svm_classifier\n",
        "    best_accuracy = svm_accuracy\n",
        "    model_name = \"Both are equal\"\n",
        "\n",
        "else:\n",
        "    best_model = rf_classifier\n",
        "    best_accuracy = rf_accuracy\n",
        "    model_name = \"Random Forest\"\n",
        "\n",
        "# Print the best model and its accuracy\n",
        "print(\"Best Model: \", model_name)\n",
        "print(\"Accuracy: \", best_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_XKXkT99x_C",
        "outputId": "e7168bf0-1e04-4a52-c449-3d6a6955114d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model:  Both are equal\n",
            "Accuracy:  1.0\n"
          ]
        }
      ]
    }
  ]
}